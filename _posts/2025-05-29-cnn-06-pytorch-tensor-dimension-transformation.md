---
title: "(CNN) PyTorch Tensor ì°¨ì› ë³€í™˜"
description: "reshape, view, permute, transposeë¥¼ í™œìš©í•œ í…ì„œ í˜•íƒœ ë³€í™˜"
date: 2025-05-29 10:00:00 +0900
categories: [Dev, CNN]
tags: [pytorch, tensor, reshape, view, permute, transpose, deep learning]
---

-------------------------------------------------------

# PyTorch Tensor ì°¨ì› ë³€í™˜

* ìµœì´ˆ ì‘ì„±ì¼: 2025ë…„ 5ì›” 29ì¼ (ëª©)

## ëª©ì°¨

1. [í…ì„œ ì°¨ì›ì˜ ì´í•´](#í…ì„œ-ì°¨ì›ì˜-ì´í•´)
2. [reshapeì™€ viewë¡œ í˜•íƒœ ë³€í™˜í•˜ê¸°](#reshapeì™€-viewë¡œ-í˜•íƒœ-ë³€í™˜í•˜ê¸°)
3. [permuteì™€ transposeë¡œ ì°¨ì› ì¬ë°°ì—´í•˜ê¸°](#permuteì™€-transposeë¡œ-ì°¨ì›-ì¬ë°°ì—´í•˜ê¸°)
4. [ì‹¤ì „ í™œìš© ì˜ˆì œ](#ì‹¤ì „-í™œìš©-ì˜ˆì œ)
5. [ì£¼ì˜ì‚¬í•­ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#ì£¼ì˜ì‚¬í•­ê³¼-ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)

---

## ğŸ“ í…ì„œ ì°¨ì›ì˜ ì´í•´

ë”¥ëŸ¬ë‹ì—ì„œ í…ì„œì˜ ì°¨ì›ì„ ì´í•´í•˜ê³  ì¡°ì‘í•˜ëŠ” ê²ƒì€ í•„ìˆ˜ì ì´ë‹¤. ê° ì°¨ì›ì€ ë°ì´í„°ì˜ íŠ¹ì • ì†ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤.

### ì°¨ì›ë³„ ì˜ë¯¸
- **1D í…ì„œ**: ë²¡í„° (ì˜ˆ: ì‹œê³„ì—´ ë°ì´í„°)
- **2D í…ì„œ**: í–‰ë ¬ (ì˜ˆ: í‘ë°± ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ì„ë² ë”©)
- **3D í…ì„œ**: íë¸Œ (ì˜ˆ: ì»¬ëŸ¬ ì´ë¯¸ì§€, ì‹œí€€ìŠ¤ ë°ì´í„°)
- **4D í…ì„œ**: ë°°ì¹˜ í¬í•¨ (ì˜ˆ: ì´ë¯¸ì§€ ë°°ì¹˜ [B, C, H, W])

```python
import torch

# 1D: ì‹œí€€ìŠ¤ ë°ì´í„°
seq = torch.arange(10)
print(f"1D tensor shape: {seq.shape}")
# ì¶œë ¥: 1D tensor shape: torch.Size([10])

# 2D: íŠ¹ì§• í–‰ë ¬
feat_mat = torch.randn(5, 3)
print(f"2D tensor shape: {feat_mat.shape}")
# ì¶œë ¥: 2D tensor shape: torch.Size([5, 3])

# 3D: RGB ì´ë¯¸ì§€
rgb_img = torch.randn(3, 224, 224)
print(f"3D tensor shape: {rgb_img.shape}")
# ì¶œë ¥: 3D tensor shape: torch.Size([3, 224, 224])

# 4D: ì´ë¯¸ì§€ ë°°ì¹˜
img_batch = torch.randn(32, 3, 224, 224)
print(f"4D tensor shape: {img_batch.shape}")
# ì¶œë ¥: 4D tensor shape: torch.Size([32, 3, 224, 224])
```

---

## ğŸ”„ reshapeì™€ viewë¡œ í˜•íƒœ ë³€í™˜í•˜ê¸°

`reshape()`ì™€ `view()` ëª¨ë‘ í…ì„œì˜ í˜•íƒœë¥¼ ë³€í™˜í•˜ì§€ë§Œ, ì¤‘ìš”í•œ ì°¨ì´ì ì´ ìˆë‹¤.

### reshape() - ìœ ì—°í•œ í˜•íƒœ ë³€í™˜
```python
# ê¸°ë³¸ reshape ì˜ˆì œ
data = torch.arange(12)
print(f"Original: {data}")
# ì¶œë ¥: Original: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])

# 2x6 í–‰ë ¬ë¡œ ë³€í™˜
mat_2x6 = data.reshape(2, 6)
print(f"2x6 matrix:\n{mat_2x6}")
# ì¶œë ¥: 2x6 matrix:
#       tensor([[ 0,  1,  2,  3,  4,  5],
#               [ 6,  7,  8,  9, 10, 11]])

# 3x4 í–‰ë ¬ë¡œ ë³€í™˜
mat_3x4 = data.reshape(3, 4)
print(f"3x4 matrix:\n{mat_3x4}")
# ì¶œë ¥: 3x4 matrix:
#       tensor([[ 0,  1,  2,  3],
#               [ 4,  5,  6,  7],
#               [ 8,  9, 10, 11]])

# -1ì„ ì‚¬ìš©í•œ ìë™ í¬ê¸° ê³„ì‚°
auto_shape = data.reshape(2, -1)
print(f"Auto-sized (2, -1): {auto_shape.shape}")
# ì¶œë ¥: Auto-sized (2, -1): torch.Size([2, 6])
```

### view() - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë³€í™˜
```python
# viewëŠ” contiguous ë©”ëª¨ë¦¬ì—ì„œë§Œ ì‘ë™
vec = torch.arange(20)
mat = vec.view(4, 5)
print(f"View result:\n{mat}")
# ì¶œë ¥: View result:
#       tensor([[ 0,  1,  2,  3,  4],
#               [ 5,  6,  7,  8,  9],
#               [10, 11, 12, 13, 14],
#               [15, 16, 17, 18, 19]])

# CNN ì¶œë ¥ì„ FC ë ˆì´ì–´ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‹¤ì „ ì˜ˆì œ
torch.manual_seed(2025)
cnn_out = torch.rand(size=(16, 64, 8, 8))  # [ë°°ì¹˜, ì±„ë„, H, W]
fc_in = cnn_out.view(16, -1)  # Flatten
print(f"CNN output: {cnn_out.shape} -> FC input: {fc_in.shape}")
# ì¶œë ¥: CNN output: torch.Size([16, 64, 8, 8]) -> FC input: torch.Size([16, 4096])
```

### Contiguous Memory ì´í•´í•˜ê¸°
```python
# ë©”ëª¨ë¦¬ ì—°ì†ì„± í™•ì¸
t1 = torch.arange(12)
t2 = t1.view(3, 4)
t3 = t1.reshape(3, 4)

print(f"Original contiguous: {t1.is_contiguous()}")
print(f"View contiguous: {t2.is_contiguous()}")
print(f"Reshape contiguous: {t3.is_contiguous()}")
# ì¶œë ¥: Original contiguous: True
#       View contiguous: True
#       Reshape contiguous: True
```

### ì˜ëª»ëœ í¬ê¸° ì§€ì • ì‹œ ì˜¤ë¥˜
```python
try:
    wrong_shape = torch.arange(10).reshape(3, 4)
except RuntimeError as e:
    print(f"Error: {e}")
# ì¶œë ¥: Error: shape '[3, 4]' is invalid for input of size 10
```

---

## ğŸ”€ permuteì™€ transposeë¡œ ì°¨ì› ì¬ë°°ì—´í•˜ê¸°

ì°¨ì›ì˜ ìˆœì„œë¥¼ ë°”ê¾¸ëŠ” ê²ƒì€ ë”¥ëŸ¬ë‹ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì‘ì—…ì´ë‹¤.

### permute() - ììœ ë¡œìš´ ì°¨ì› ì¬ë°°ì—´
```python
# ì´ë¯¸ì§€ ë°ì´í„° í˜•ì‹ ë³€í™˜: HWC -> CHW
torch.manual_seed(2025)
img_hwc = torch.rand(size=(224, 224, 3))  # Height, Width, Channel
print(f"HWC format: {img_hwc.shape}")
# ì¶œë ¥: HWC format: torch.Size([224, 224, 3])

img_chw = img_hwc.permute(2, 0, 1)  # Channel, Height, Width
print(f"CHW format: {img_chw.shape}")
# ì¶œë ¥: CHW format: torch.Size([3, 224, 224])

# ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ ë³€í™˜
img_back = img_chw.permute(1, 2, 0)
print(f"Back to HWC: {img_back.shape}")
# ì¶œë ¥: Back to HWC: torch.Size([224, 224, 3])

# 4D í…ì„œ ì˜ˆì œ: NHWC -> NCHW (ë°°ì¹˜ í¬í•¨)
batch_nhwc = torch.rand(size=(32, 224, 224, 3))
batch_nchw = batch_nhwc.permute(0, 3, 1, 2)
print(f"NHWC: {batch_nhwc.shape} -> NCHW: {batch_nchw.shape}")
# ì¶œë ¥: NHWC: torch.Size([32, 224, 224, 3]) -> NCHW: torch.Size([32, 3, 224, 224])
```

### transpose() - ë‘ ì°¨ì› êµí™˜
```python
# íŠ¹ì • ë‘ ì°¨ì›ë§Œ êµí™˜
feat_map = torch.rand(size=(64, 3, 128, 256))
print(f"Original shape: {feat_map.shape}")
# ì¶œë ¥: Original shape: torch.Size([64, 3, 128, 256])

# ë†’ì´ì™€ ë„ˆë¹„ êµí™˜
transposed = feat_map.transpose(2, 3)
print(f"After transpose(2, 3): {transposed.shape}")
# ì¶œë ¥: After transpose(2, 3): torch.Size([64, 3, 256, 128])

# ì—°ì†ì ì¸ transpose
double_trans = feat_map.transpose(1, 2).transpose(2, 3)
print(f"Double transpose: {double_trans.shape}")
# ì¶œë ¥: Double transpose: torch.Size([64, 128, 3, 256])
```

### t() - 2D í…ì„œ ì „ì¹˜
```python
# 2ì°¨ì› í–‰ë ¬ ì „ì¹˜
matrix = torch.rand(size=(3, 5))
print(f"Original matrix:\n{matrix}")
# ì¶œë ¥: Original matrix:
#       tensor([[0.5234, 0.3456, 0.7890, 0.1234, 0.9876],
#               [0.2345, 0.6789, 0.3456, 0.8901, 0.4567],
#               [0.8901, 0.2345, 0.5678, 0.3456, 0.7890]])

transposed = matrix.t()
print(f"Transposed shape: {transposed.shape}")
# ì¶œë ¥: Transposed shape: torch.Size([5, 3])
```

### Contiguous ë©”ëª¨ë¦¬ ë¬¸ì œì™€ í•´ê²°
```python
# permute í›„ contiguous ë©”ëª¨ë¦¬ êµ¬ì¡° í™•ì¸
img = torch.rand(size=(3, 64, 64))
img_perm = img.permute(1, 2, 0)
print(f"After permute, contiguous: {img_perm.is_contiguous()}")
# ì¶œë ¥: After permute, contiguous: False

# view ì‹œë„ ì‹œ ì˜¤ë¥˜ ë°œìƒ
try:
    img_flat = img_perm.view(64, -1)
except RuntimeError as e:
    print(f"View error: {e}")
# ì¶œë ¥: View error: view size is not compatible with input tensor's size and stride

# í•´ê²°ë°©ë²• 1: reshape ì‚¬ìš©
img_flat_reshape = img_perm.reshape(64, -1)
print(f"Reshape success: {img_flat_reshape.shape}")
# ì¶œë ¥: Reshape success: torch.Size([64, 192])

# í•´ê²°ë°©ë²• 2: contiguous() í›„ view ì‚¬ìš©
img_flat_view = img_perm.contiguous().view(64, -1)
print(f"Contiguous + view success: {img_flat_view.shape}")
# ì¶œë ¥: Contiguous + view success: torch.Size([64, 192])
```

---

## ğŸ’¼ ì‹¤ì „ í™œìš© ì˜ˆì œ

### 1. CNNì—ì„œ FC ë ˆì´ì–´ë¡œ ì „í™˜
```python
# CNN ë§ˆì§€ë§‰ ë ˆì´ì–´ ì¶œë ¥ì„ Fully Connected ì…ë ¥ìœ¼ë¡œ ë³€í™˜
class CNNToFC(torch.nn.Module):
    def forward(self, x):
        # x shape: [batch, channels, height, width]
        batch_size = x.shape[0]
        # Flatten ë°©ë²• 1: view
        x_flat_1 = x.view(batch_size, -1)
        # Flatten ë°©ë²• 2: reshape
        x_flat_2 = x.reshape(batch_size, -1)
        return x_flat_1

# ì˜ˆì œ ì‹¤í–‰
model = CNNToFC()
cnn_output = torch.randn(32, 512, 7, 7)
fc_input = model(cnn_output)
print(f"CNN output: {cnn_output.shape} -> FC input: {fc_input.shape}")
# ì¶œë ¥: CNN output: torch.Size([32, 512, 7, 7]) -> FC input: torch.Size([32, 25088])
```

### 2. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ìœ„í•œ ì°¨ì› ë³€í™˜
```python
# Multi-head attentionì„ ìœ„í•œ ì°¨ì› ì¬ë°°ì—´
def prepare_attention(x, num_heads=8):
    # x shape: [batch, seq_len, embed_dim]
    b, s, d = x.shape
    head_dim = d // num_heads
    
    # [batch, seq_len, num_heads, head_dim]ìœ¼ë¡œ reshape
    x = x.reshape(b, s, num_heads, head_dim)
    
    # [batch, num_heads, seq_len, head_dim]ìœ¼ë¡œ permute
    x = x.permute(0, 2, 1, 3)
    
    return x

# ì˜ˆì œ ì‹¤í–‰
seq_data = torch.randn(16, 100, 512)  # [batch, seq_len, embed_dim]
attn_input = prepare_attention(seq_data, num_heads=8)
print(f"Original: {seq_data.shape} -> Attention: {attn_input.shape}")
# ì¶œë ¥: Original: torch.Size([16, 100, 512]) -> Attention: torch.Size([16, 8, 100, 64])
```

### 3. ì´ë¯¸ì§€ ë°ì´í„° í˜•ì‹ ë³€í™˜
```python
# PyTorch (NCHW) <-> TensorFlow (NHWC) í˜•ì‹ ë³€í™˜
def pytorch_to_tensorflow(img_batch):
    # NCHW -> NHWC
    return img_batch.permute(0, 2, 3, 1)

def tensorflow_to_pytorch(img_batch):
    # NHWC -> NCHW
    return img_batch.permute(0, 3, 1, 2)

# ì˜ˆì œ ì‹¤í–‰
pytorch_batch = torch.randn(8, 3, 224, 224)  # PyTorch í˜•ì‹
tf_batch = pytorch_to_tensorflow(pytorch_batch)
print(f"PyTorch: {pytorch_batch.shape} -> TensorFlow: {tf_batch.shape}")
# ì¶œë ¥: PyTorch: torch.Size([8, 3, 224, 224]) -> TensorFlow: torch.Size([8, 224, 224, 3])

back_to_pytorch = tensorflow_to_pytorch(tf_batch)
print(f"Back to PyTorch: {back_to_pytorch.shape}")
# ì¶œë ¥: Back to PyTorch: torch.Size([8, 3, 224, 224])
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤
```python
# view vs reshape ì„±ëŠ¥ ë¹„êµ
large_tensor = torch.randn(1000, 1000)

# viewëŠ” ë©”ëª¨ë¦¬ ê³µìœ  (ë” íš¨ìœ¨ì )
view_result = large_tensor.view(100, 10000)
print(f"Same storage: {view_result.data_ptr() == large_tensor.data_ptr()}")
# ì¶œë ¥: Same storage: True

# permute í›„ì—ëŠ” ì£¼ì˜ í•„ìš”
permuted = large_tensor.t()
print(f"After transpose, contiguous: {permuted.is_contiguous()}")
# ì¶œë ¥: After transpose, contiguous: False
```

### 2. ì°¨ì› ë³€í™˜ ì²´í¬ë¦¬ìŠ¤íŠ¸
- âœ… ë³€í™˜ ì „í›„ ì›ì†Œ ê°œìˆ˜ê°€ ë™ì¼í•œì§€ í™•ì¸
- âœ… contiguous ë©”ëª¨ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° í™•ì¸
- âœ… ì°¨ì›ì˜ ì˜ë¯¸ê°€ ì˜¬ë°”ë¥´ê²Œ ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸
- âœ… ë°°ì¹˜ ì°¨ì›ì€ ë³´í†µ ì²« ë²ˆì§¸ ì°¨ì›ìœ¼ë¡œ ìœ ì§€

### 3. ë””ë²„ê¹… íŒ
```python
def debug_tensor_shape(tensor, name="tensor"):
    print(f"{name}:")
    print(f"  Shape: {tensor.shape}")
    print(f"  Size: {tensor.numel()}")
    print(f"  Contiguous: {tensor.is_contiguous()}")
    print(f"  Device: {tensor.device}")

# ì‚¬ìš© ì˜ˆ
test_tensor = torch.randn(2, 3, 4).permute(2, 0, 1)
debug_tensor_shape(test_tensor, "Permuted tensor")
# ì¶œë ¥: Permuted tensor:
#         Shape: torch.Size([4, 2, 3])
#         Size: 24
#         Contiguous: False
#         Device: cpu
```

---

## ğŸ“Š í•¨ìˆ˜ë³„ ìš”ì•½

| í•¨ìˆ˜ | ìš©ë„ | íŠ¹ì§• | ì£¼ì˜ì‚¬í•­ |
|------|------|------|----------|
| `reshape` | í˜•íƒœ ë³€í™˜ | ìœ ì—°í•¨, ìƒˆ ë©”ëª¨ë¦¬ í• ë‹¹ ê°€ëŠ¥ | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ |
| `view` | í˜•íƒœ ë³€í™˜ | ë©”ëª¨ë¦¬ íš¨ìœ¨ì  | contiguous í•„ìš” |
| `permute` | ì°¨ì› ì¬ë°°ì—´ | ëª¨ë“  ì°¨ì› ììœ ë¡­ê²Œ | contiguous ê¹¨ì§ |
| `transpose` | ë‘ ì°¨ì› êµí™˜ | íŠ¹ì • ë‘ ì°¨ì›ë§Œ | contiguous ê¹¨ì§ |
| `t` | 2D ì „ì¹˜ | 2ì°¨ì› ì „ìš© | ê°„ë‹¨í•œ í–‰ë ¬ ì „ì¹˜ |

ì´ ê°€ì´ë“œë¥¼ í†µí•´ PyTorch í…ì„œì˜ ì°¨ì› ë³€í™˜ì„ ììœ ìì¬ë¡œ ë‹¤ë£° ìˆ˜ ìˆê²Œ ë˜ê¸°ë¥¼ ë°”ë€ë‹¤!
