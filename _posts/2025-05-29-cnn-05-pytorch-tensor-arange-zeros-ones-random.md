---
title: "(CNN) arange, zeros, ones, random"
description: "arangeì™€ ê¸°ë³¸ í…ì„œ í•¨ìˆ˜ ë° ë‚œìˆ˜ ìƒì„± í•¨ìˆ˜"
date: 2025-05-29 10:00:00 +0900
categories: [Dev, CNN]
tags: [pytorch, tensor, random, arange, deep learning, initialization]
---

-------------------------------------------------------

# arange, zeros, onesì™€ random

* ìµœì´ˆ ì‘ì„±ì¼: 2025ë…„ 5ì›” 29ì¼ (ëª©)

## ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [arange - ì—°ì†ëœ ê°’ìœ¼ë¡œ í…ì„œ ìƒì„±](#arange---ì—°ì†ëœ-ê°’ìœ¼ë¡œ-í…ì„œ-ìƒì„±)
3. [zeros & ones - ì´ˆê¸°í™”ëœ í…ì„œ ìƒì„±](#zeros--ones---ì´ˆê¸°í™”ëœ-í…ì„œ-ìƒì„±)
4. [ë‚œìˆ˜ í…ì„œ ìƒì„±](#ë‚œìˆ˜-í…ì„œ-ìƒì„±)
5. [ì‹¤ì „ í™œìš© íŒ](#ì‹¤ì „-í™œìš©-íŒ)

---

## ğŸ“š ê°œìš”

PyTorchì—ì„œ í…ì„œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œì˜ ì²«ê±¸ìŒì´ë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” í…ì„œ ìƒì„± í•¨ìˆ˜ë“¤ì„ ì‹¤ìš©ì ì¸ ì˜ˆì œì™€ í•¨ê»˜ ì†Œê°œí•œë‹¤.

**ì–¸ì œ ì–´ë–¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í• ê¹Œ?**
- `arange`: ì¸ë±ìŠ¤ ìƒì„±, ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬
- `zeros/ones`: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, ë§ˆìŠ¤í¬ ìƒì„±
- `rand/randn`: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, ë°ì´í„° ì¦ê°•
- `randint`: ë°°ì¹˜ ìƒ˜í”Œë§, ë ˆì´ë¸” ìƒì„±

---

## ğŸ”¢ arange - ì—°ì†ëœ ê°’ìœ¼ë¡œ í…ì„œ ìƒì„±

`torch.arange()`ëŠ” íŒŒì´ì¬ì˜ `range()`ì™€ ìœ ì‚¬í•˜ê²Œ ì—°ì†ëœ ìˆ«ìë“¤ì˜ í…ì„œë¥¼ ìƒì„±í•œë‹¤.

### í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜
```python
torch.arange(start=0, end, step=1, *, out=None, dtype=None, 
             layout=torch.strided, device=None, requires_grad=False)
```

### ì£¼ìš” íŒŒë¼ë¯¸í„°
- **start**: ì‹œì‘ê°’ (ê¸°ë³¸ê°’: 0)
- **end**: ì¢…ë£Œê°’ (**ë¯¸í¬í•¨**)
- **step**: ì¦ê°€ ê°„ê²© (ê¸°ë³¸ê°’: 1)
- **dtype**: ë°ì´í„° íƒ€ì… (ìë™ ì¶”ë¡ ë¨)

### ì‹¤ì „ ì˜ˆì œ

**ê¸°ë³¸ ì‚¬ìš©ë²•**
```python
# 0ë¶€í„° 9ê¹Œì§€ì˜ ì •ìˆ˜
seq_tensor = torch.arange(10)
print(seq_tensor)
# ì¶œë ¥: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

# íŠ¹ì • ë²”ìœ„ ì§€ì •
custom_range = torch.arange(start=2, end=9, step=2)
print(custom_range)
# ì¶œë ¥: tensor([2, 4, 6, 8])

# ì‹¤ìˆ˜í˜• í…ì„œ ìƒì„±
float_range = torch.arange(0, 1, 0.2)
print(float_range)
# ì¶œë ¥: tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000])
```

**ì‹¤ìš©ì  í™œìš©: ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±**
```python
# Transformer ëª¨ë¸ì˜ ìœ„ì¹˜ ì¸ì½”ë”©ì— í™œìš©
seq_length = 100
position = torch.arange(0, seq_length).unsqueeze(1)
print(f"Position encoding shape: {position.shape}")
# ì¶œë ¥: Position encoding shape: torch.Size([100, 1])
print(position[:5])  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
# ì¶œë ¥: tensor([[0],
#              [1],
#              [2],
#              [3],
#              [4]])
```

---

## ğŸ§± zeros & ones - ì´ˆê¸°í™”ëœ í…ì„œ ìƒì„±

íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ì›Œì§„ í…ì„œë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í•œë‹¤.

### í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜
```python
torch.zeros(*size, out=None, dtype=None, layout=torch.strided, 
            device=None, requires_grad=False)

torch.ones(*size, out=None, dtype=None, layout=torch.strided, 
           device=None, requires_grad=False)
```

### ì£¼ìš” íŒŒë¼ë¯¸í„°
- **size**: í…ì„œì˜ shape (íŠœí”Œ ë˜ëŠ” ê°œë³„ ì¸ì)
- **dtype**: ë°ì´í„° íƒ€ì… (ê¸°ë³¸ê°’: float32)
- **device**: CPU ë˜ëŠ” GPU ì§€ì •

### ì‹¤ì „ ì˜ˆì œ

**zeros í™œìš©: íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±**
```python
# ë°°ì¹˜ í¬ê¸° 32, ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ 50ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬
batch_size, max_len = 32, 50
padding_mask = torch.zeros(batch_size, max_len, dtype=torch.bool)
print(f"Padding mask shape: {padding_mask.shape}")
# ì¶œë ¥: Padding mask shape: torch.Size([32, 50])
print(padding_mask[0, :10])  # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì²˜ìŒ 10ê°œ ê°’
# ì¶œë ¥: tensor([False, False, False, False, False, False, False, False, False, False])

# 3ì°¨ì› í…ì„œ ìƒì„±
zero_3d = torch.zeros(2, 3, 4)
print(zero_3d)
# ì¶œë ¥: tensor([[[0., 0., 0., 0.],
#               [0., 0., 0., 0.],
#               [0., 0., 0., 0.]],
#              [[0., 0., 0., 0.],
#               [0., 0., 0., 0.],
#               [0., 0., 0., 0.]]])
```

**ones í™œìš©: Attention ë§ˆìŠ¤í¬ ì´ˆê¸°í™”**
```python
# Self-attentionì„ ìœ„í•œ ì´ˆê¸° ë§ˆìŠ¤í¬
seq_len = 5
attention_mask = torch.ones(seq_len, seq_len)
print(attention_mask)
# ì¶œë ¥: tensor([[1., 1., 1., 1., 1.],
#              [1., 1., 1., 1., 1.],
#              [1., 1., 1., 1., 1.],
#              [1., 1., 1., 1., 1.],
#              [1., 1., 1., 1., 1.]])

# ì •ìˆ˜í˜• ones í…ì„œ
int_ones = torch.ones(3, 3, dtype=torch.int64)
print(int_ones)
# ì¶œë ¥: tensor([[1, 1, 1],
#              [1, 1, 1],
#              [1, 1, 1]])
```

**GPU ë©”ëª¨ë¦¬ í• ë‹¹**
```python
# GPUê°€ ìˆë‹¤ë©´ ë°”ë¡œ GPUì— í…ì„œ ìƒì„±
if torch.cuda.is_available():
    gpu_tensor = torch.zeros(3, 3, device='cuda')
    print(f"Tensor device: {gpu_tensor.device}")
    # ì¶œë ¥: Tensor device: cuda:0
else:
    cpu_tensor = torch.zeros(3, 3, device='cpu')
    print(f"Tensor device: {cpu_tensor.device}")
    # ì¶œë ¥: Tensor device: cpu
```

---

## ğŸ² ë‚œìˆ˜ í…ì„œ ìƒì„±

ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì™€ ë°ì´í„° ì¦ê°•ì— í•„ìˆ˜ì ì¸ ë‚œìˆ˜ ìƒì„± í•¨ìˆ˜ë“¤ì´ë‹¤.

### ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •
```python
torch.manual_seed(42)  # CPU ì‹œë“œ
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)  # GPU ì‹œë“œ
```

### 1ï¸âƒ£ torch.rand() - ê· ì¼ ë¶„í¬ [0, 1)

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: Dropout ë§ˆìŠ¤í¬, í™•ë¥ ì  ìƒ˜í”Œë§

```python
# ê¸°ë³¸ ì‚¬ìš©ë²•
uniform_tensor = torch.rand(3, 4)
print(uniform_tensor)
# ì¶œë ¥: tensor([[0.8823, 0.9150, 0.3829, 0.9593],
#              [0.3904, 0.6009, 0.2566, 0.7936],
#              [0.9408, 0.1332, 0.9346, 0.5936]])

# Dropout êµ¬í˜„ ì˜ˆì‹œ
dropout_rate = 0.5
features = torch.randn(4, 5)  # ë°°ì¹˜ í¬ê¸° 4, íŠ¹ì§• ì°¨ì› 5
dropout_mask = torch.rand(4, 5) > dropout_rate
print(dropout_mask)
# ì¶œë ¥: tensor([[ True,  True, False,  True, False],
#              [False,  True, False,  True,  True],
#              [ True, False,  True,  True, False],
#              [ True,  True, False, False,  True]])

output = features * dropout_mask
print(f"Active neurons: {dropout_mask.sum().item()}/{dropout_mask.numel()}")
# ì¶œë ¥: Active neurons: 12/20
```

### 2ï¸âƒ£ torch.randn() - í‘œì¤€ ì •ê·œ ë¶„í¬ N(0, 1)

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, ë…¸ì´ì¦ˆ ì¶”ê°€

```python
# ê¸°ë³¸ ì‚¬ìš©ë²•
normal_tensor = torch.randn(3, 3)
print(normal_tensor)
# ì¶œë ¥: tensor([[-0.1276,  0.5846, -0.8667],
#              [-0.2233,  1.4459, -0.2951],
#              [-0.8948, -0.0125, -1.2220]])

print(f"Mean: {normal_tensor.mean():.4f}, Std: {normal_tensor.std():.4f}")
# ì¶œë ¥: Mean: -0.1611, Std: 0.8436

# Xavier ì´ˆê¸°í™” êµ¬í˜„
input_dim, output_dim = 4, 3
xavier_std = (2.0 / (input_dim + output_dim)) ** 0.5
weights = torch.randn(input_dim, output_dim) * xavier_std
print(weights)
# ì¶œë ¥: tensor([[ 0.5456, -0.4515,  0.6135],
#              [-0.0812, -0.5416,  0.0402],
#              [ 0.4672,  0.7812, -0.2051],
#              [-0.5897, -0.1279,  0.4492]])
print(f"Xavier weight stats - Mean: {weights.mean():.4f}, Std: {weights.std():.4f}")
# ì¶œë ¥: Xavier weight stats - Mean: 0.0622, Std: 0.4477
```

**ë°ì´í„° ì¦ê°•: ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€**
```python
# ì‘ì€ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆ ì¶”ê°€ (2x2 ì´ë¯¸ì§€ ì˜ˆì‹œ)
images = torch.rand(2, 2)
print("Original image:")
print(images)
# ì¶œë ¥: Original image:
#       tensor([[0.4963, 0.7682],
#               [0.0885, 0.1320]])

noise_level = 0.1
noise = torch.randn_like(images) * noise_level
noisy_images = images + noise
noisy_images = torch.clamp(noisy_images, 0, 1)  # [0, 1] ë²”ìœ„ë¡œ ì œí•œ
print("Noisy image:")
print(noisy_images)
# ì¶œë ¥: Noisy image:
#       tensor([[0.5427, 0.8145],
#               [0.0943, 0.1893]])
```

### 3ï¸âƒ£ torch.randint() - ì •ìˆ˜ ê· ì¼ ë¶„í¬

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: í´ë˜ìŠ¤ ë ˆì´ë¸” ìƒì„±, ì¸ë±ìŠ¤ ìƒ˜í”Œë§

```python
# ë¶„ë¥˜ ë¬¸ì œë¥¼ ìœ„í•œ ëœë¤ ë ˆì´ë¸” ìƒì„±
num_classes = 5
batch_size = 10
random_labels = torch.randint(0, num_classes, (batch_size,))
print(random_labels)
# ì¶œë ¥: tensor([4, 4, 3, 4, 4, 3, 3, 1, 3, 2])

unique_vals, counts = random_labels.unique(return_counts=True)
print(f"Label distribution: values={unique_vals.tolist()}, counts={counts.tolist()}")
# ì¶œë ¥: Label distribution: values=[1, 2, 3, 4], counts=[1, 1, 4, 4]

# 2ì°¨ì› ì •ìˆ˜ í…ì„œ
int_matrix = torch.randint(low=10, high=20, size=(3, 4))
print(int_matrix)
# ì¶œë ¥: tensor([[16, 17, 10, 18],
#              [11, 14, 12, 17],
#              [18, 13, 14, 16]])

# ë°°ì¹˜ì—ì„œ ëœë¤ ìƒ˜í”Œ ì„ íƒ
total_samples = 100
batch_size = 5
batch_indices = torch.randint(0, total_samples, (batch_size,))
print(f"Selected indices: {batch_indices}")
# ì¶œë ¥: Selected indices: tensor([84, 26, 80, 55, 72])
```

---

## ğŸ’¡ ì‹¤ì „ í™œìš© íŒ

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í…ì„œ ìƒì„±
```python
# í° í…ì„œëŠ” dtypeì„ ëª…ì‹œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
large_tensor = torch.zeros(1000, 1000, dtype=torch.float16)
memory_mb = large_tensor.element_size() * large_tensor.numel() / 1024**2
print(f"Memory usage: {memory_mb:.1f} MB")
# ì¶œë ¥: Memory usage: 1.9 MB

# float32ì™€ ë¹„êµ
large_tensor_32 = torch.zeros(1000, 1000, dtype=torch.float32)
memory_mb_32 = large_tensor_32.element_size() * large_tensor_32.numel() / 1024**2
print(f"Memory usage (float32): {memory_mb_32:.1f} MB")
# ì¶œë ¥: Memory usage (float32): 3.8 MB
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¸Œë¡œë“œìºìŠ¤íŒ… í™œìš©
```python
# ë°°ì¹˜ì˜ ê° ìƒ˜í”Œì— ë‹¤ë¥¸ ë°”ì´ì–´ìŠ¤ ì¶”ê°€
batch_data = torch.randn(3, 4)  # 3ê°œ ìƒ˜í”Œ, 4ì°¨ì›
bias = torch.arange(4).float()  # 4ì°¨ì› ë°”ì´ì–´ìŠ¤
print("Batch data:")
print(batch_data)
# ì¶œë ¥: Batch data:
#       tensor([[-0.1115,  0.1204, -0.3696, -0.2404],
#               [-1.1969, -0.1097,  1.1050, -1.5701],
#               [ 0.4927,  0.7854, -0.4551,  0.7581]])

print("Bias:", bias)
# ì¶œë ¥: Bias: tensor([0., 1., 2., 3.])

result = batch_data + bias  # ìë™ ë¸Œë¡œë“œìºìŠ¤íŒ…
print("Result after adding bias:")
print(result)
# ì¶œë ¥: Result after adding bias:
#       tensor([[-0.1115,  1.1204,  1.6304,  2.7596],
#               [-1.1969,  0.8903,  3.1050,  1.4299],
#               [ 0.4927,  1.7854,  1.5449,  3.7581]])
```

### 3. ê°™ì€ shapeì˜ í…ì„œ ìƒì„±
```python
# ê¸°ì¡´ í…ì„œì™€ ê°™ì€ shapeë¡œ ìƒì„±
reference = torch.randn(2, 3)
print(f"Reference shape: {reference.shape}")
# ì¶œë ¥: Reference shape: torch.Size([2, 3])

zeros_like = torch.zeros_like(reference)
print("zeros_like:")
print(zeros_like)
# ì¶œë ¥: zeros_like:
#       tensor([[0., 0., 0.],
#               [0., 0., 0.]])

ones_like = torch.ones_like(reference)
print("ones_like:")
print(ones_like)
# ì¶œë ¥: ones_like:
#       tensor([[1., 1., 1.],
#               [1., 1., 1.]])

randn_like = torch.randn_like(reference)
print("randn_like:")
print(randn_like)
# ì¶œë ¥: randn_like:
#       tensor([[-0.5531,  1.3090, -0.2774],
#               [ 0.7712, -0.3763, -0.7993]])
```

### 4. ë””ë°”ì´ìŠ¤ ê°„ ì´ë™ ìµœì†Œí™”
```python
# GPUì—ì„œ ì§ì ‘ ìƒì„±í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³µì‚¬ ë°©ì§€
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
# ì¶œë ¥: Using device: cuda (ë˜ëŠ” cpu)

# ì²˜ìŒë¶€í„° ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ìƒì„±
tensor = torch.randn(3, 3, device=device)
print(f"Tensor is on: {tensor.device}")
# ì¶œë ¥: Tensor is on: cuda:0 (ë˜ëŠ” cpu)

# ì˜ëª»ëœ ë°©ë²• (í”¼í•´ì•¼ í•¨)
# bad_tensor = torch.randn(3, 3)  # CPUì— ìƒì„±
# bad_tensor = bad_tensor.to(device)  # GPUë¡œ ë³µì‚¬ (ì¶”ê°€ ì‹œê°„ ì†Œìš”)
```

---

## ğŸ“Š í•¨ìˆ˜ë³„ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½

| í•¨ìˆ˜ | ì£¼ìš” ìš©ë„ | ì˜ˆì‹œ |
|------|-----------|------|
| `arange` | ì¸ë±ìŠ¤, ì‹œí€€ìŠ¤ ìƒì„± | ìœ„ì¹˜ ì¸ì½”ë”©, ì‹œê°„ ìŠ¤í… |
| `zeros` | ì´ˆê¸°í™”, ë§ˆìŠ¤í‚¹ | íŒ¨ë”© ë§ˆìŠ¤í¬, ëˆ„ì  ë³€ìˆ˜ |
| `ones` | ì´ˆê¸°í™”, ë§ˆìŠ¤í‚¹ | Attention ë§ˆìŠ¤í¬, ë°”ì´ì–´ìŠ¤ ì´ˆê¸°í™” |
| `rand` | í™•ë¥ ì  ì²˜ë¦¬ | Dropout, ë°ì´í„° ìƒ˜í”Œë§ |
| `randn` | ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” | Xavier/He ì´ˆê¸°í™”, ë…¸ì´ì¦ˆ |
| `randint` | ì´ì‚° ìƒ˜í”Œë§ | ë ˆì´ë¸” ìƒì„±, ì¸ë±ì‹± |

ì´ ê°€ì´ë“œë¥¼ í†µí•´ PyTorchì˜ í…ì„œ ìƒì„± í•¨ìˆ˜ë“¤ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë” ë‚˜ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ë°”ë€ë‹¤!
